{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test databricks-connect\n",
    "\n",
    "requirements:\n",
    "\n",
    "- service principal\n",
    "- secrets in databricks\n",
    "- venv\n",
    "- databricks profiles\n",
    "- GRANT SELECT ON ANY FILE TO `service principal id`\n",
    "- external location\n",
    "- GRANT CREATE EXTERNAL TABLE ON external location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade \"databricks-connect==15.4.*\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from pyspark.sql.types import *\n",
    "from databricks.connect import DatabricksSession\n",
    "from databricks.sdk import WorkspaceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create workspace client\n",
    "w = WorkspaceClient(profile='WORKSPACEM2M')\n",
    "\n",
    "#create spark session\n",
    "spark = DatabricksSession.builder.profile('WORKSPACEM2M').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageAccountName = \"adedlspws\"\n",
    "containerName = \"azadescontainer\"\n",
    "directoryName = \"raw\"\n",
    "\n",
    "#create dbutils \n",
    "dbutils = w.dbutils\n",
    "\n",
    "#get credentials\n",
    "accountKey = dbutils.secrets.get(scope=\"azurestorage\", key=\"accountkey\")\n",
    "principalid = dbutils.secrets.get(scope=\"workspacem2m\", key=\"principalid\")\n",
    "\n",
    "# Configure Spark to access Azure Blob storage account\n",
    "spark.conf.set(f\"fs.azure.account.key.{storageAccountName}.dfs.core.windows.net\", accountKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEMP VIEW AND CTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the schema for the orders file\n",
    "\n",
    "orderschema = StructType(\n",
    "    [\n",
    "        StructField(\"invoiceno\", StringType(), True),\n",
    "        StructField(\"stockcode\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"quantity\", IntegerType(), True),\n",
    "        StructField(\"invoicedate\", StringType(), True),\n",
    "        StructField(\"unitprice\", DoubleType(), True),\n",
    "        StructField(\"customerid\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"orderid\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# read the orders files from the datain folder in orders container\n",
    "\n",
    "ordersdf = spark.read.csv(\n",
    "    f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/{directoryName}/\",\n",
    "    schema=orderschema,\n",
    "    sep=\",\",\n",
    ")\n",
    "\n",
    "#create view to access the dataframe in sql context\n",
    "ordersdf.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+-------+\n",
      "|invoiceno|stockcode|         description|quantity|        invoicedate|unitprice|customerid|Country|orderid|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+-------+\n",
      "|   536370|    10002|INFLATABLE POLITI...|      48|2010-12-01 08:45:00|     0.85|         4| France|   1467|\n",
      "|   536382|    10002|INFLATABLE POLITI...|      12|2010-12-01 09:45:00|     0.85|         4|  India|   1578|\n",
      "|   536756|    10002|INFLATABLE POLITI...|       1|2010-12-02 14:23:00|     0.85|         1|Bahrain|   5708|\n",
      "|   536863|    10002|INFLATABLE POLITI...|       1|2010-12-03 11:19:00|     0.85|         7|Bahrain|   6902|\n",
      "|   536865|    10002|INFLATABLE POLITI...|       5|2010-12-03 11:28:00|     1.66|         1|Bahrain|   6982|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from orders\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#location and credential\n",
    "\n",
    "metastore = \"abfss://ade-metastore@adedlspws.dfs.core.windows.net/\"\n",
    "catalogue = \"adedatabricks\"\n",
    "database = \"sales\"\n",
    "table = \"orders_stage\"\n",
    "external_location_name = \"azade-storage-location\"\n",
    "location = f\"{metastore}/{database}/stage\"\n",
    "storage_credential = \"azade-storage-credential\"\n",
    "\n",
    "#drop db\n",
    "spark.sql(f\"DROP DATABASE IF EXISTS {catalogue}.{database} CASCADE;\")\n",
    "\n",
    "#create db\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalogue}.{database};\")\n",
    "\n",
    "#create table\n",
    "query = f'''\n",
    "CREATE OR REPLACE TABLE {catalogue}.{database}.{table}\n",
    "USING delta\n",
    "LOCATION '{location}'\n",
    "AS SELECT *\n",
    "FROM orders;\n",
    "'''\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+-------+\n",
      "|invoiceno|stockcode|         description|quantity|        invoicedate|unitprice|customerid|Country|orderid|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+-------+\n",
      "|   536370|    10002|INFLATABLE POLITI...|      48|2010-12-01 08:45:00|     0.85|         4| France|   1467|\n",
      "|   536382|    10002|INFLATABLE POLITI...|      12|2010-12-01 09:45:00|     0.85|         4|  India|   1578|\n",
      "|   536756|    10002|INFLATABLE POLITI...|       1|2010-12-02 14:23:00|     0.85|         1|Bahrain|   5708|\n",
      "|   536863|    10002|INFLATABLE POLITI...|       1|2010-12-03 11:19:00|     0.85|         7|Bahrain|   6902|\n",
      "|   536865|    10002|INFLATABLE POLITI...|       5|2010-12-03 11:28:00|     1.66|         1|Bahrain|   6982|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query the external location\n",
    "\n",
    "spark.sql(\"SELECT * FROM adedatabricks.sales.orders_stage\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COPY INTO, INSERT AND MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "\n",
    "storage_credential = \"azade-storage-credential\"\n",
    "metastore = \"abfss://ade-metastore@adedlspws.dfs.core.windows.net/\"\n",
    "catalogue = \"adedatabricks\"\n",
    "database = \"sales\"\n",
    "external_location_name = \"azade-storage-location\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create landing table\n",
    "landing_table = \"orders_landing\"\n",
    "landing_location = f\"{metastore}/{database}/landing\"\n",
    "\n",
    "#create table\n",
    "query = f'''\n",
    "CREATE TABLE {catalogue}.{database}.{landing_table}(\n",
    "_c0 string,\n",
    "_c1 string,\n",
    "_c2 string,\n",
    "_c3 string,\n",
    "_c4 string,\n",
    "_c5 string,\n",
    "_c6 string,\n",
    "_c7 string,\n",
    "_c8 string\n",
    ")\n",
    "USING delta\n",
    "location '{landing_location}';\n",
    "'''\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create staging table\n",
    "stage_table = \"orders_stage\"\n",
    "stage_location = f\"{metastore}/{database}/stage\"\n",
    "\n",
    "\n",
    "#drop db\n",
    "spark.sql(f\"DROP DATABASE IF EXISTS {catalogue}.{database} CASCADE;\")\n",
    "\n",
    "#create db\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalogue}.{database};\")\n",
    "\n",
    "\n",
    "#create table\n",
    "query = f'''\n",
    "CREATE OR REPLACE TABLE {catalogue}.{database}.{stage_table} (\n",
    "    invoiceno STRING,\n",
    "    stockcode STRING,\n",
    "    description STRING,\n",
    "    quantity INT,\n",
    "    invoicedate STRING,\n",
    "    unitprice DOUBLE,\n",
    "    customerid STRING,\n",
    "    country STRING,\n",
    "    orderid INT\n",
    ")\n",
    "USING delta\n",
    "OPTIONS (overwriteSchema = 'true')\n",
    "LOCATION '{stage_location}';\n",
    "'''\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+-------+\n",
      "|invoiceNo|stockCode|description|quantity|invoiceDate|unitPrice|customerID|country|orderid|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create orders table\n",
    "bronze_table = \"orders_bronze\"\n",
    "bronze_location = f\"{metastore}/{database}/bronze\"\n",
    "\n",
    "query = f'''\n",
    "CREATE TABLE IF NOT EXISTS {catalogue}.{database}.{bronze_table} (\n",
    "    invoiceno string,\n",
    "    stockcode string,\n",
    "    description string,\n",
    "    quantity int,\n",
    "    invoicedate string,\n",
    "    unitprice string,\n",
    "    customerid string,\n",
    "    country string,\n",
    "    orderid int\n",
    ")\n",
    "USING delta\n",
    "LOCATION '{bronze_location}';\n",
    "'''\n",
    "spark.sql(query)\n",
    "\n",
    "# check orders_bronze table\n",
    "spark.sql(f\"SELECT * FROM {catalogue}.{database}.{bronze_table}\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+---+-------------------+----+---+-------+----+\n",
      "|   _c0|  _c1|                 _c2|_c3|                _c4| _c5|_c6|    _c7| _c8|\n",
      "+------+-----+--------------------+---+-------------------+----+---+-------+----+\n",
      "|536370|10002|INFLATABLE POLITI...| 48|2010-12-01 08:45:00|0.85|  4| France|1467|\n",
      "|536382|10002|INFLATABLE POLITI...| 12|2010-12-01 09:45:00|0.85|  4|  India|1578|\n",
      "|536756|10002|INFLATABLE POLITI...|  1|2010-12-02 14:23:00|0.85|  1|Bahrain|5708|\n",
      "|536863|10002|INFLATABLE POLITI...|  1|2010-12-03 11:19:00|0.85|  7|Bahrain|6902|\n",
      "|536865|10002|INFLATABLE POLITI...|  5|2010-12-03 11:28:00|1.66|  1|Bahrain|6982|\n",
      "+------+-----+--------------------+---+-------------------+----+---+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COPY INTO ingest for landing table\n",
    "\n",
    "source_path = f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/{directoryName}/\"\n",
    "\n",
    "query = f'''\n",
    "    COPY INTO {catalogue}.{database}.{landing_table}\n",
    "    FROM '{source_path}'\n",
    "    FILEFORMAT = CSV\n",
    "    PATTERN = '*.csv*'\n",
    "    FORMAT_OPTIONS ('delimiter'=',','header'='false');\n",
    "'''\n",
    "spark.sql(query)\n",
    "\n",
    "# check the table\n",
    "spark.sql(f\"SELECT * FROM {catalogue}.{database}.{landing_table}\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+-------+\n",
      "|invoiceno|stockcode|         description|quantity|        invoicedate|unitprice|customerid|country|orderid|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+-------+\n",
      "|   536370|    10002|INFLATABLE POLITI...|      48|2010-12-01 08:45:00|     0.85|         4| France|   1467|\n",
      "|   536382|    10002|INFLATABLE POLITI...|      12|2010-12-01 09:45:00|     0.85|         4|  India|   1578|\n",
      "|   536756|    10002|INFLATABLE POLITI...|       1|2010-12-02 14:23:00|     0.85|         1|Bahrain|   5708|\n",
      "|   536863|    10002|INFLATABLE POLITI...|       1|2010-12-03 11:19:00|     0.85|         7|Bahrain|   6902|\n",
      "|   536865|    10002|INFLATABLE POLITI...|       5|2010-12-03 11:28:00|     1.66|         1|Bahrain|   6982|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INSERT INTO stage table\n",
    "\n",
    "query = f''' \n",
    "INSERT INTO {catalogue}.{database}.{stage_table}\n",
    "SELECT * FROM {catalogue}.{database}.{landing_table};\n",
    "'''\n",
    "spark.sql(query)\n",
    "\n",
    "# check the table\n",
    "spark.sql(f\"SELECT * FROM {catalogue}.{database}.{stage_table}\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEAN AND MERGE\n",
    "\n",
    "clean, transform and ingest the data into the orders table.\n",
    "1. Update country \"Unspecified\" to \"India\"\n",
    "2. Update customerid to 1 where it's null\n",
    "3. Merge orders from orders_staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+-------+\n",
      "|invoiceNo|stockCode|         description|quantity|        invoiceDate|unitPrice|customerID|country|orderid|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+-------+\n",
      "|   578539|    11001|ASSTD DESIGN RACI...|      16|2011-11-24 14:55:00|     1.69|         1|  India|  56619|\n",
      "|   578539|    20749|ASSORTED COLOUR M...|       4|2011-11-24 14:55:00|     7.95|         1|  India|  56603|\n",
      "|   578539|    20750|RED RETROSPOT MIN...|       4|2011-11-24 14:55:00|     7.95|         1|  India|  56604|\n",
      "|   578539|    20975|12 PENCILS SMALL ...|      24|2011-11-24 14:55:00|     0.65|         1|  India|  56611|\n",
      "|   578539|    20981|12 PENCILS TALL T...|      12|2011-11-24 14:55:00|     0.85|         1|  India|  56612|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clean stage_table\n",
    "\n",
    "clean_country = f\"UPDATE {catalogue}.{database}.{stage_table} SET country='India' where country='Unspecified';\"\n",
    "spark.sql(clean_country)\n",
    "\n",
    "clean_customer = f\"UPDATE {catalogue}.{database}.{stage_table} SET customerid='1' where customerid='null';\"\n",
    "spark.sql(clean_customer)\n",
    "\n",
    "# MERGE INTO bronze table\n",
    "\n",
    "merge_query = f'''\n",
    "    MERGE INTO {catalogue}.{database}.{bronze_table} as orders\n",
    "    USING {catalogue}.{database}.{stage_table} as stage\n",
    "    ON\n",
    "    (\n",
    "        orders.InvoiceNo = stage.invoiceno AND \n",
    "        orders.stockCode = stage.stockcode AND \n",
    "        orders.customerID = stage.customerid\n",
    "    )\n",
    "    WHEN MATCHED THEN \n",
    "    UPDATE SET \n",
    "        orders.unitPrice=stage.unitprice,\n",
    "        orders.quantity=stage.quantity,\n",
    "        orders.country=stage.country\n",
    "    \n",
    "    WHEN NOT MATCHED THEN \n",
    "    INSERT\n",
    "    (\n",
    "        invoiceNo,\n",
    "        stockCode,\n",
    "        description,\n",
    "        quantity,\n",
    "        invoiceDate,\n",
    "        unitPrice,\n",
    "        customerID,\n",
    "        country,\n",
    "        orderid\n",
    "    )\n",
    "    \n",
    "    VALUES\n",
    "    (\n",
    "        invoiceno,\n",
    "        stockcode,\n",
    "        description,\n",
    "        quantity,\n",
    "        invoicedate, \n",
    "        unitprice,\n",
    "        customerid,\n",
    "        country,\n",
    "        orderid\n",
    "    )\n",
    "'''\n",
    "spark.sql(merge_query)\n",
    "\n",
    "# check the table\n",
    "spark.sql(f\"SELECT * FROM {catalogue}.{database}.{bronze_table}\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORMATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the customer information from SQL Database\n",
    "\n",
    "#SQL Server FQDN\n",
    "jdbcHostname = \"azadesqlserver.database.windows.net\" \n",
    "jdbcDatabase = \"azadesqldb\" \n",
    "jdbcPort = 1433\n",
    "\n",
    "#get username and password for database\n",
    "username = dbutils.secrets.get(scope = \"azuresql\", key = \"username\")\n",
    "password = dbutils.secrets.get(scope = \"azuresql\", key = \"password\")\n",
    "\n",
    "# set the JDBC URL\n",
    "jdbcUrl = \\\n",
    "\"jdbc:sqlserver://{0}:{1};database={2};user={3};password={4}\".format(\n",
    "    jdbcHostname, jdbcPort, jdbcDatabase, username, password)\n",
    "\n",
    "# read customer table.\n",
    "customerdf = spark.read.jdbc(url=jdbcUrl, table=\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|     name|\n",
      "+---+---------+\n",
      "|  1|   Hyphen|\n",
      "|  2|     Page|\n",
      "|  3| Data Inc|\n",
      "|  4|    Delta|\n",
      "|  5|     Genx|\n",
      "|  6| Rand Inc|\n",
      "|  7|Hallo Inc|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join ordersdf and customerdf and select only the required columns\n",
    "customerordersdf = ordersdf \\\n",
    "    .join(customerdf, ordersdf.customerid == customerdf.id, how=\"inner\") \\\n",
    "    .select(ordersdf.Country, ordersdf.unitprice, ordersdf.quantity, customerdf.name.alias(\"customername\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+--------+------------+\n",
      "|       Country|unitprice|quantity|customername|\n",
      "+--------------+---------+--------+------------+\n",
      "|United Kingdom|     0.95|       6|      Hyphen|\n",
      "|United Kingdom|     1.65|       1|      Hyphen|\n",
      "|United Kingdom|      0.0|    -110|      Hyphen|\n",
      "|United Kingdom|      0.0|     170|      Hyphen|\n",
      "|United Kingdom|      0.0|    -178|      Hyphen|\n",
      "|United Kingdom|      0.0|      60|      Hyphen|\n",
      "|United Kingdom|      0.0|      70|      Hyphen|\n",
      "|United Kingdom|     2.95|       2|      Hyphen|\n",
      "|United Kingdom|     5.95|       2|      Hyphen|\n",
      "|United Kingdom|     5.95|       2|      Hyphen|\n",
      "|United Kingdom|     1.25|       3|      Hyphen|\n",
      "|United Kingdom|     0.85|       3|      Hyphen|\n",
      "|United Kingdom|   201.37|       1|      Hyphen|\n",
      "|United Kingdom|     0.83|       1|      Hyphen|\n",
      "|United Kingdom|     0.83|       2|      Hyphen|\n",
      "|United Kingdom|     0.83|       1|      Hyphen|\n",
      "|United Kingdom|     0.83|       1|      Hyphen|\n",
      "|United Kingdom|     0.83|       2|      Hyphen|\n",
      "|United Kingdom|     0.83|       2|      Hyphen|\n",
      "|United Kingdom|     0.83|       1|      Hyphen|\n",
      "+--------------+---------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerordersdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fliter data\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "filterdf = customerordersdf \\\n",
    "    .filter((col(\"country\")!=\"Unspecified\") & (col(\"quantity\")>0)) \\\n",
    "    .withColumn(\"totalamount\", (col(\"unitprice\")*col(\"quantity\")) \\\n",
    "    .cast(\"decimal(10,2)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+--------+------------+-----------+\n",
      "|       Country|unitprice|quantity|customername|totalamount|\n",
      "+--------------+---------+--------+------------+-----------+\n",
      "|United Kingdom|     0.95|       6|      Hyphen|       5.70|\n",
      "|United Kingdom|     1.65|       1|      Hyphen|       1.65|\n",
      "|United Kingdom|      0.0|     170|      Hyphen|       0.00|\n",
      "|United Kingdom|      0.0|      60|      Hyphen|       0.00|\n",
      "|United Kingdom|      0.0|      70|      Hyphen|       0.00|\n",
      "|United Kingdom|     2.95|       2|      Hyphen|       5.90|\n",
      "|United Kingdom|     5.95|       2|      Hyphen|      11.90|\n",
      "|United Kingdom|     5.95|       2|      Hyphen|      11.90|\n",
      "|United Kingdom|     1.25|       3|      Hyphen|       3.75|\n",
      "|United Kingdom|     0.85|       3|      Hyphen|       2.55|\n",
      "|United Kingdom|   201.37|       1|      Hyphen|     201.37|\n",
      "|United Kingdom|     0.83|       1|      Hyphen|       0.83|\n",
      "|United Kingdom|     0.83|       2|      Hyphen|       1.66|\n",
      "|United Kingdom|     0.83|       1|      Hyphen|       0.83|\n",
      "|United Kingdom|     0.83|       1|      Hyphen|       0.83|\n",
      "|United Kingdom|     0.83|       2|      Hyphen|       1.66|\n",
      "|United Kingdom|     0.83|       2|      Hyphen|       1.66|\n",
      "|United Kingdom|     0.83|       1|      Hyphen|       0.83|\n",
      "|United Kingdom|     2.46|       4|      Hyphen|       9.84|\n",
      "|United Kingdom|     8.32|       1|      Hyphen|       8.32|\n",
      "+--------------+---------+--------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "salesbycountries = filterdf.groupBy(\"customername\", \"country\").agg(f.sum(\"totalamount\").alias(\"Amount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+------------------+\n",
      "|customername|             country|            Amount|\n",
      "+------------+--------------------+------------------+\n",
      "|   Hallo Inc|         Netherlands| 58330.76000000004|\n",
      "|        Page|      United Kingdom|7856.5099999999975|\n",
      "|   Hallo Inc|             Austria| 961.6000000000001|\n",
      "|   Hallo Inc|             Denmark|           1680.72|\n",
      "|   Hallo Inc|United Arab Emirates| 864.0400000000002|\n",
      "|   Hallo Inc|              Poland|           1465.43|\n",
      "|   Hallo Inc|         Switzerland| 8869.329999999994|\n",
      "|   Hallo Inc|               Japan|          16869.82|\n",
      "|   Hallo Inc|              Greece|2982.8500000000004|\n",
      "|      Hyphen|           Hong Kong|2419.9600000000005|\n",
      "|      Hyphen|                EIRE| 3310.999999999998|\n",
      "|   Hallo Inc|             Finland| 7694.349999999999|\n",
      "|        Page|             Germany| 816.2500000000001|\n",
      "|   Hallo Inc|           Singapore|           1987.37|\n",
      "|   Hallo Inc|             Germany| 48481.56999999994|\n",
      "|   Hallo Inc|              Cyprus| 7323.350000000001|\n",
      "|   Hallo Inc|              Norway|10150.660000000007|\n",
      "|   Hallo Inc|        Saudi Arabia|            145.92|\n",
      "|   Hallo Inc|           Lithuania|1661.0600000000006|\n",
      "|   Hallo Inc|               Spain|13533.070000000007|\n",
      "+------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesbycountries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+------------------+\n",
      "|customername|        country|            Amount|\n",
      "+------------+---------------+------------------+\n",
      "|   Hallo Inc| United Kingdom| 1584382.439999807|\n",
      "|      Hyphen| United Kingdom| 537848.9699999607|\n",
      "|   Hallo Inc|           EIRE| 58479.49000000003|\n",
      "|   Hallo Inc|    Netherlands| 58330.76000000004|\n",
      "|   Hallo Inc|        Germany| 48481.56999999994|\n",
      "|   Hallo Inc|         France|45537.200000000004|\n",
      "|   Hallo Inc|      Australia| 40499.27000000001|\n",
      "|   Hallo Inc|          Japan|          16869.82|\n",
      "|   Hallo Inc|          Spain|13533.070000000007|\n",
      "|       Delta| United Kingdom|11934.840000000007|\n",
      "|   Hallo Inc|         Norway|10150.660000000007|\n",
      "|   Hallo Inc|       Portugal| 9269.690000000002|\n",
      "|   Hallo Inc|    Switzerland| 8869.329999999994|\n",
      "|        Page| United Kingdom|7856.5099999999975|\n",
      "|   Hallo Inc|         Sweden| 7833.720000000001|\n",
      "|   Hallo Inc|        Finland| 7694.349999999999|\n",
      "|   Hallo Inc|         Cyprus| 7323.350000000001|\n",
      "|   Hallo Inc|        Belgium|6399.7499999999945|\n",
      "|   Hallo Inc|Channel Islands| 6312.149999999998|\n",
      "|    Rand Inc| United Kingdom| 6225.479999999993|\n",
      "+------------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#insert data into SalesStaging table.\n",
    "\n",
    "salesbycountries.write.jdbc(jdbcUrl,\"dbo.SalesStaging\", mode=\"overwrite\")\n",
    "\n",
    "#read the SalesStaging table. \n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "salesstagingdf = spark.read.jdbc(url=jdbcUrl, table=\"SalesStaging\")\n",
    "\n",
    "salesstagingdf.sort(desc(\"Amount\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
